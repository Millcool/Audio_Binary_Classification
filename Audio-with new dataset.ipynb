{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T22:09:46.171054Z",
     "iopub.status.busy": "2022-07-13T22:09:46.170371Z",
     "iopub.status.idle": "2022-07-13T22:09:49.059971Z",
     "shell.execute_reply": "2022-07-13T22:09:49.058765Z",
     "shell.execute_reply.started": "2022-07-13T22:09:46.170925Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from IPython import display as ipd\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import ImageFolder,  DatasetFolder,VisionDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import torchvision.models as models\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torchaudio\n",
    "#from pydub import AudioSegment\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    JUST_PREDICT  = False\n",
    "    Kaggle        = False \n",
    "    DEBUG         = False\n",
    "    FULL_DATA     = True\n",
    "    wandb_on      = False\n",
    "    seed          = 101\n",
    "    MULTIMODEL    = False\n",
    "    weights       = 'imagenet'\n",
    "    backbone      = 'efficientnet-b1'\n",
    "    archive_name  = 'Audio'\n",
    "    models        = []\n",
    "    optimizers    = []\n",
    "################################################### \n",
    "    num_of_models = 1\n",
    "    model_number  = 1\n",
    "    train_bs      = 32\n",
    "    valid_bs      = 32\n",
    "    SAMPLE_RATE = 16000\n",
    "    NUM_SAMPLES = 48000\n",
    "    number_imgs   = 10 if DEBUG else 649     #8203\n",
    "    num_test      = 10 if DEBUG else 301      # 1000\n",
    "    print_every   = 1  if DEBUG else 50      #500\n",
    "    epochs        = 2  if DEBUG else 22        #35\n",
    "    ###############################################\n",
    "    crop_koef     = 1\n",
    "    lr            = 0.002\n",
    "    num_workers   = 4 if Kaggle else 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler     = 'CosineAnnealingLR'\n",
    "    min_lr        = 1e-6\n",
    "    T_max         = int(30000/train_bs*epochs)+50\n",
    "    T_0           = 25\n",
    "    warmup_epochs = 0\n",
    "    wd            = 1e-6\n",
    "    n_accumulate  = max(1, 32//train_bs)\n",
    "    n_fold        = 5\n",
    "    num_classes   = 2\n",
    "    classes       = [0,1]\n",
    "    activation    = None #'softmax'\n",
    "    device        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "    images_path   = \"../input/russian-railways-2/images/images/\" if Kaggle else \"../../Desktop/hack/image\"\n",
    "    masks_path    = \"../input/russian-railways-2/mask/mask/\" if Kaggle else  \"../../Desktop/hack/masks\"\n",
    "    test_path     = \"../input/russian-railways-2/test/test/\" if Kaggle else \"./test/\"\n",
    "    save_path     = '../working/result/' if Kaggle else \"./result/\"\n",
    "    train_path    = '../../Stagirovki/YandexML_TEST/train/' \n",
    "    csv_path      = '../../Stagirovki/YandexML_TEST/targets.tsv'\n",
    "    best_model_w  = '../input/russian-railways-2/best_epoch_ofu-efficientnet-b4_v2.bin' if Kaggle else f'./best_epoch_ofu-{backbone}_v2.bin'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Instalations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T22:09:49.069029Z",
     "iopub.status.busy": "2022-07-13T22:09:49.065209Z",
     "iopub.status.idle": "2022-07-13T22:09:49.171598Z",
     "shell.execute_reply": "2022-07-13T22:09:49.165850Z",
     "shell.execute_reply.started": "2022-07-13T22:09:49.068991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#waveform, sample_rate = torchaudio.load('./train/000ad36ce0dcbc1032a606312d5e787d.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display.Audio(waveform, rate = sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#waveform.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mel_Spectrogram = torchaudio.transforms.MelSpectrogram()(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mel_Spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PySoundFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T22:09:49.214518Z",
     "iopub.status.busy": "2022-07-13T22:09:49.214144Z",
     "iopub.status.idle": "2022-07-13T22:09:49.225837Z",
     "shell.execute_reply": "2022-07-13T22:09:49.224029Z",
     "shell.execute_reply.started": "2022-07-13T22:09:49.214485Z"
    }
   },
   "outputs": [],
   "source": [
    "# Different classes dataset. \n",
    "classes = ('0','1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T22:09:49.227391Z",
     "iopub.status.busy": "2022-07-13T22:09:49.226911Z",
     "iopub.status.idle": "2022-07-13T22:09:49.245425Z",
     "shell.execute_reply": "2022-07-13T22:09:49.243813Z",
     "shell.execute_reply.started": "2022-07-13T22:09:49.227332Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv(CFG.csv_path, sep = '\\\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={0: \"Name\", 1: \"Target\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>path</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DF_E_2000053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DF_E_2000058</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DF_E_2000079</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DF_E_2000246</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name Target\n",
       "0          path   fake\n",
       "1  DF_E_2000053      0\n",
       "2  DF_E_2000058      0\n",
       "3  DF_E_2000079      0\n",
       "4  DF_E_2000246      0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchaudio.io import StreamReader\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# audio_ids = list(data[1])\n",
    "# audio_names = list(data[0])\n",
    "# print(audio_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                lable = False):\n",
    "        self.annotations = annotations_file\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.lable = lable\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_dir)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print(index)\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        #signal = self._resample_if_necessary(signal, sr)\n",
    "        #signal = self._cut_if_necessary(signal)\n",
    "        #signal = self._right_pad_if_necessary(signal)\n",
    "        #signal = signal.repeat(3, 1, 1)\n",
    "        #signal = torch.squeeze(signal)\n",
    "        #signal = self.transformation(signal)\n",
    "        if self.lable == True: # WHEN WE TRAIN\n",
    "            label = self._get_audio_sample_label(index)\n",
    "            return signal, label\n",
    "        else: # WHEN WE PREDICT\n",
    "            return signal, torch.randint(0, 1, (1,))\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, : self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        path = self.audio_dir[index]\n",
    "        if self.lable == True:\n",
    "            path = os.path.join(\"../../Stagirovki/YandexML_TEST/train/\",path)\n",
    "        else:\n",
    "            path = os.path.join(\"../../Stagirovki/YandexML_TEST/train/\",path)\n",
    "        path = path + '.wav'\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        path = self.audio_dir[index]\n",
    "        #print(path)\n",
    "        df = self.annotations\n",
    "        df = df.loc[lambda df: df['Name'] == path]\n",
    "        #print(df.head())\n",
    "        num = list(df['Target'])\n",
    "        #print(num)\n",
    "        return torch.Tensor(num)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     ANNOTATIONS_FILE = \"/home/valerio/datasets/UrbanSound8K/metadata/UrbanSound8K.csv\"\n",
    "#     AUDIO_DIR = \"/home/valerio/datasets/UrbanSound8K/audio\"\n",
    "#     SAMPLE_RATE = 22050\n",
    "#     NUM_SAMPLES = 22050\n",
    "\n",
    "#     mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "#         sample_rate=SAMPLE_RATE,\n",
    "#         n_fft=1024,\n",
    "#         hop_length=512,\n",
    "#         n_mels=64\n",
    "#     )\n",
    "\n",
    "#     usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n",
    "#                             AUDIO_DIR,\n",
    "#                             mel_spectrogram,\n",
    "#                             SAMPLE_RATE,\n",
    "#                             NUM_SAMPLES)\n",
    "#     print(f\"There are {len(usd)} samples in the dataset.\")\n",
    "#     signal, label = usd[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clearing_names(audio_names : list, data) :\n",
    "#     for audio in enumerate(audio_names):\n",
    "#         if audio not in data:\n",
    "#             audio_names.remove(audio)\n",
    "#     return audio_names, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_loaders():\n",
    "    \n",
    "    audio_names= [item_name.split('.')[0] for item_name in os.listdir(CFG.train_path)] #[os.path.join(\"./train/\",item_name)  for item_name in os.listdir(CFG.train_path)]\n",
    "    #print(audio_names)\n",
    "    audio_train, audio_valid = train_test_split(audio_names, test_size=0.2 , random_state=42)\n",
    "    #print(type(train_ids))\n",
    "    #print(valid_ids)\n",
    "    train_dataset = UrbanSoundDataset(data, audio_train, mel_spectrogram, CFG.SAMPLE_RATE, CFG.NUM_SAMPLES, True)\n",
    "    valid_dataset = UrbanSoundDataset(data, audio_valid, mel_spectrogram, CFG.SAMPLE_RATE, CFG.NUM_SAMPLES, True)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=CFG.train_bs, num_workers=0, shuffle=True, pin_memory=True,collate_fn=Collator(), drop_last=False)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=CFG.valid_bs, num_workers=0, shuffle=False,collate_fn=Collator(), pin_memory=True)\n",
    "    \n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T22:09:49.253393Z",
     "iopub.status.busy": "2022-07-13T22:09:49.253083Z",
     "iopub.status.idle": "2022-07-13T22:09:49.294497Z",
     "shell.execute_reply": "2022-07-13T22:09:49.292752Z",
     "shell.execute_reply.started": "2022-07-13T22:09:49.253358Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# transformations = transforms.Compose(\n",
    "#     ProcessChannels(mode=avg)\n",
    "#     AdditiveNoise(prob=0.3, sig=0.001, dist_type=normal)\n",
    "#     RandomCropLength(prob=0.4, sig=0.25, dist_type=half)\n",
    "#     ToTensorAudio()\n",
    "# )\n",
    "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=CFG.SAMPLE_RATE,\n",
    "    n_fft=1024,\n",
    "    win_length = 1024,\n",
    "    hop_length=256,\n",
    "    n_mels=80,\n",
    "    window_fn = torch.hann_window,\n",
    "    center=True\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Melspectrogram_visualize(array: torch.Tensor):\n",
    "    #print(array.shape)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.imshow(array.squeeze().log())\n",
    "    plt.xlabel('Time', size=20)\n",
    "    plt.ylabel('Frequency (Hz)', size=20)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    \n",
    "def visualize_audio(wav: torch.Tensor, sr: int = 22050):\n",
    "    # Average all channels\n",
    "    if wav.dim() == 2:\n",
    "        # Any to mono audio convertion\n",
    "        wav = wav.mean(dim=0)\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.plot(wav, alpha=.7, c='green')\n",
    "    plt.grid()\n",
    "    plt.xlabel('Time', size=20)\n",
    "    plt.ylabel('Amplitude', size=20)\n",
    "    plt.show()\n",
    "    \n",
    "    display.display(display.Audio(wav, rate=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List , Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collator:\n",
    "    def __call__(self, batch: List[Tuple[torch.Tensor, int]]):\n",
    "        lengths = []\n",
    "        wavs, labels = zip(*batch)\n",
    "        \n",
    "        for wav in wavs:\n",
    "            lengths.append(wav.size(-1))\n",
    "        \n",
    "        # here we should pad wvas to one length, cause we need pass it to network\n",
    "        batch_wavs = torch.zeros(len(batch), max(lengths))\n",
    "        for i, wav in enumerate(wavs):\n",
    "            batch_wavs[i, :lengths[i]]=wav\n",
    "            \n",
    "        \n",
    "        \n",
    "        labels = torch.tensor(labels).long()\n",
    "        lengths = torch.tensor(lengths).long()\n",
    "        \n",
    "        return {\n",
    "            'wav': batch_wavs,\n",
    "            'label': labels,\n",
    "            'length': lengths,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = prepare_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15296, 3824)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader) , len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(audio[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36mUrbanSoundDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#signal = self._resample_if_necessary(signal, sr)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#signal = self._cut_if_necessary(signal)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#signal = self._right_pad_if_necessary(signal)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#signal = signal.repeat(3, 1, 1)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#signal = torch.squeeze(signal)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#signal = self.transformation(signal)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlable \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m: \u001b[38;5;66;03m# WHEN WE TRAIN\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_audio_sample_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m signal, label\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# WHEN WE PREDICT\u001b[39;00m\n",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36mUrbanSoundDataset._get_audio_sample_label\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     75\u001b[0m num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m#print(num)\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "audio = next(iter(train_loader))\n",
    "print(audio['wav'].shape)\n",
    "#print(Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melspectrogram_visualize(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Featurizer(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Featurizer, self).__init__()\n",
    "        \n",
    "        self.featurizer = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=16_000,\n",
    "            n_fft=1024,\n",
    "            win_length=1024,\n",
    "            hop_length=256,\n",
    "            n_mels=64,\n",
    "            center=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, wav, length=None):\n",
    "        mel_spectrogram = self.featurizer(wav)\n",
    "        mel_spectrogram = mel_spectrogram.clamp(min=1e-5).log()\n",
    "        \n",
    "        if length is not None:\n",
    "            length = (length - self.featurizer.win_length) // self.featurizer.hop_length\n",
    "            # We add `4` because in MelSpectrogram center==True\n",
    "            length += 1 + 4\n",
    "            \n",
    "            return mel_spectrogram, length\n",
    "        \n",
    "        return mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_size): #64,128\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.LSTM(input_size=input_dim,\n",
    "                           hidden_size=hidden_size,\n",
    "                           num_layers = 1,\n",
    "                           batch_first=True,\n",
    "                           bidirectional = False)\n",
    "        self.clf = nn.Linear(hidden_size, 2)\n",
    "    \n",
    "    def forward(self, input, length=None):\n",
    "        print(input.transpose(-1, -2).shape)\n",
    "        # input: (batch_size, hidden_size, seq_len)\n",
    "        output, _ = self.rnn(input.transpose(-1, -2))\n",
    "        # output: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Now we want to take the last hidden state of each instance in batch\n",
    "        # BUT we don't want to take `padding` hidden state\n",
    "        # We will use `torch.gather` and `length` to dio that\n",
    "        \n",
    "        # learn more about gather\n",
    "        # https://medium.com/analytics-vidhya/understanding-indexing-with-pytorch-gather-33717a84ebc4\n",
    "        index = length.sub(1).view(-1, 1, 1).expand(-1, -1, self.hidden_size)\n",
    "        print(f'index{index}')\n",
    "        last_hidden = torch.gather( #  ,берем в результат число стоящее в той же полосе/столбце по индексу указанному в index\n",
    "            output, # мы так убираем один dim \n",
    "            dim =1, # index - массив из одинаковых элементов равных максимальной длине вектора в батче\n",
    "            index = length.sub(1).view(-1, 1, 1).expand(-1, -1, self.hidden_size) # view (-1,1,1) выытягивает все значения в линию- тензор\n",
    "        )\n",
    "        \n",
    "        \n",
    "        logits = self.clf(last_hidden.squeeze(dim=1))\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = Model(input_dim=64, hidden_size=128).to(device)\n",
    "featurizer = Featurizer().to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcef5d835076466283f09820af84aa6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15296 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m train_loss_meter \u001b[38;5;241m=\u001b[39m AverageMeter()\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_loader)):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Move batch to device if device != 'cpu'\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     wav \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwav\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m     length \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\notebook.py:258\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36mUrbanSoundDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#signal = self._resample_if_necessary(signal, sr)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#signal = self._cut_if_necessary(signal)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#signal = self._right_pad_if_necessary(signal)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#signal = signal.repeat(3, 1, 1)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#signal = torch.squeeze(signal)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#signal = self.transformation(signal)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlable \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m: \u001b[38;5;66;03m# WHEN WE TRAIN\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_audio_sample_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m signal, label\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# WHEN WE PREDICT\u001b[39;00m\n",
      "Input \u001b[1;32mIn [51]\u001b[0m, in \u001b[0;36mUrbanSoundDataset._get_audio_sample_label\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     75\u001b[0m num \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m#print(num)\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "for epoch in range(CFG.epochs):\n",
    "    train_loss_meter = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        # Move batch to device if device != 'cpu'\n",
    "        wav = batch['wav'].to(device)\n",
    "        length = batch['length'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "\n",
    "        mel, mel_length = featurizer(wav, length)\n",
    "        #print(mel.shape ,mel_length)\n",
    "        output = model(mel, mel_length)\n",
    "        print(output.shape)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_meter.update(loss.item())\n",
    "        \n",
    "    storage['train_loss'].append(train_loss_meter.avg)\n",
    "    \n",
    "    validation_loss_meter = AverageMeter()\n",
    "    validation_accuracy_meter = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "    for i, batch in enumerate(tqdm(valid_loader)):\n",
    "        # Move batch to device if device != 'cpu'\n",
    "        wav = batch['wav'].to(device)\n",
    "        length = batch['length'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            mel, mel_length = featurizer(wav, length)\n",
    "            output = model(mel, mel_length)\n",
    "\n",
    "            loss = criterion(output, label)\n",
    "        \n",
    "        matches = (output.argmax(dim=-1) == label).float().mean()\n",
    "\n",
    "        validation_loss_meter.update(loss.item())\n",
    "        validation_accuracy_meter.update(matches.item())\n",
    "        \n",
    "    storage['validation_loss'].append(validation_loss_meter.avg)\n",
    "    storage['validation_accuracy'].append(validation_accuracy_meter.avg)\n",
    "    \n",
    "    display.clear_output()\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].plot(storage['train_loss'], label='train_loss')\n",
    "    axes[1].plot(storage['validation_loss'], label='validation_loss')\n",
    "\n",
    "    axes[2].plot(storage['validation_accuracy'], label='validation_accuracy')\n",
    "\n",
    "    for i in range(3):\n",
    "        axes[i].grid()\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T22:11:13.108463Z",
     "iopub.status.busy": "2022-07-13T22:11:13.107767Z",
     "iopub.status.idle": "2022-07-13T22:27:41.170460Z",
     "shell.execute_reply": "2022-07-13T22:27:41.169446Z",
     "shell.execute_reply.started": "2022-07-13T22:11:13.108421Z"
    }
   },
   "outputs": [],
   "source": [
    "#fit(model, CFG.epochs , train_loader, valid_loader, optimizer, CFG.criterion, CFG.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Efficcient_net predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T22:27:41.922371Z",
     "iopub.status.busy": "2022-07-13T22:27:41.921713Z",
     "iopub.status.idle": "2022-07-13T22:27:45.531639Z",
     "shell.execute_reply": "2022-07-13T22:27:45.530687Z",
     "shell.execute_reply.started": "2022-07-13T22:27:41.922318Z"
    }
   },
   "outputs": [],
   "source": [
    "#test = []\n",
    "'./test/'\n",
    "items_names= [item_name.split('.')[0] for item_name in os.listdir(CFG.test_path)]#os.path.join(\"./test\",item_name)\n",
    "test_dataset = UrbanSoundDataset(data, items_names, mel_spectrogram, CFG.SAMPLE_RATE, CFG.NUM_SAMPLES, False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, num_workers=0, shuffle=False, pin_memory=True, drop_last=False,collate_fn=Collator())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i, batch in enumerate(tqdm(test_loader)):\n",
    "        # Move batch to device if device != 'cpu'\n",
    "        wav = batch['wav'].to(device)\n",
    "        length = batch['length'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            mel, mel_length = featurizer(wav, length)\n",
    "            output = model(mel, mel_length)\n",
    "            #print(output)\n",
    "            _, pred = torch.max(output, 1) # Return values and indices\n",
    "            #m = nn.Softmax(dim=1)\n",
    "            #pred = m(output)\n",
    "            #print(pred)\n",
    "            #print(pred.item())\n",
    "            #df\n",
    "            test.append(classes[pred.item()])\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_names= [item_name.split('.')[0] for item_name in os.listdir(CFG.test_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-13T22:29:38.581683Z",
     "iopub.status.busy": "2022-07-13T22:29:38.581326Z",
     "iopub.status.idle": "2022-07-13T22:29:38.588448Z",
     "shell.execute_reply": "2022-07-13T22:29:38.585882Z",
     "shell.execute_reply.started": "2022-07-13T22:29:38.581655Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\" :items_names,\n",
    "                   \"eff\": test\n",
    "        \n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T22:27:45.747598Z",
     "iopub.status.idle": "2022-07-13T22:27:45.748300Z",
     "shell.execute_reply": "2022-07-13T22:27:45.748059Z",
     "shell.execute_reply.started": "2022-07-13T22:27:45.748035Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "ansamble_pred = df['eff'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Generating submission file...')\n",
    "#os.chdir(\"./\")\n",
    "df.to_csv('answers.tsv',index=False, sep = '\\t',header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T22:27:45.749586Z",
     "iopub.status.idle": "2022-07-13T22:27:45.750270Z",
     "shell.execute_reply": "2022-07-13T22:27:45.750041Z",
     "shell.execute_reply.started": "2022-07-13T22:27:45.750017Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\"id\" : range(1,250),\n",
    "#                    \"class\": ansamble_pred\n",
    "# #                   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T22:27:45.751572Z",
     "iopub.status.idle": "2022-07-13T22:27:45.752256Z",
     "shell.execute_reply": "2022-07-13T22:27:45.752029Z",
     "shell.execute_reply.started": "2022-07-13T22:27:45.752006Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({\"id\" : range(1,250),\n",
    "#                    \"class\": test\n",
    "#                   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T22:27:45.754037Z",
     "iopub.status.idle": "2022-07-13T22:27:45.754749Z",
     "shell.execute_reply": "2022-07-13T22:27:45.754517Z",
     "shell.execute_reply.started": "2022-07-13T22:27:45.754493Z"
    }
   },
   "outputs": [],
   "source": [
    "# df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T22:27:45.756008Z",
     "iopub.status.idle": "2022-07-13T22:27:45.756719Z",
     "shell.execute_reply": "2022-07-13T22:27:45.756489Z",
     "shell.execute_reply.started": "2022-07-13T22:27:45.756465Z"
    }
   },
   "outputs": [],
   "source": [
    "# df.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T22:27:45.757997Z",
     "iopub.status.idle": "2022-07-13T22:27:45.758717Z",
     "shell.execute_reply": "2022-07-13T22:27:45.758484Z",
     "shell.execute_reply.started": "2022-07-13T22:27:45.758460Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_full.tail(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T22:27:45.760001Z",
     "iopub.status.idle": "2022-07-13T22:27:45.760725Z",
     "shell.execute_reply": "2022-07-13T22:27:45.760495Z",
     "shell.execute_reply.started": "2022-07-13T22:27:45.760471Z"
    }
   },
   "outputs": [],
   "source": [
    "# print('Generating submission file...')\n",
    "#os.chdir(\"./\")\n",
    "# df.to_csv('submission_audio.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-07-13T22:27:45.761988Z",
     "iopub.status.idle": "2022-07-13T22:27:45.762720Z",
     "shell.execute_reply": "2022-07-13T22:27:45.762480Z",
     "shell.execute_reply.started": "2022-07-13T22:27:45.762456Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_full.to_csv('submission_effnet7new2.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
